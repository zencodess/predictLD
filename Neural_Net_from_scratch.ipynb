{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named matplotlib.pyplot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e0e1492b7973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named matplotlib.pyplot"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "hidden_layers = 2\n",
    "nodes_in_layer = [10, 5]\n",
    "number_of_training_iterations = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "XOR2_16nm_stat00.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9c6eae31b2ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Data loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmy_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'XOR2_16nm_stat00.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# data = np.genfromtxt(cStringIO.StringIO(data),dtype = \"float\", delimiter=\",\", names = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sathya/.local/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   1687\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1689\u001b[0;31m             \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1690\u001b[0m             \u001b[0mown_fhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sathya/.local/lib/python2.7/site-packages/numpy/lib/_datasource.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sathya/.local/lib/python2.7/site-packages/numpy/lib/_datasource.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    614\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    615\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: XOR2_16nm_stat00.csv not found."
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "# Data loading\n",
    "\n",
    "my_data = np.genfromtxt('XOR2_16nm_stat00.csv', delimiter=',',skip_header=1,dtype = 'float')\n",
    "data = my_data\n",
    "# data = np.genfromtxt(cStringIO.StringIO(data),dtype = \"float\", delimiter=\",\", names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cStringIO\n",
    "# with open (\"XOR2_16nm_stat00.csv\", \"r\") as myfile:\n",
    "#        data = myfile.read().replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length = data.shape[0]\n",
    "# feature_count = data.shape[1]\n",
    "# \n",
    "# train_data = data\n",
    "\n",
    "# train_data_inputs = train_data[:, :feature_count - 3]\n",
    "# train_data_outputs = train_data[:, feature_count - 3: feature_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test_Sampling\n",
    "# train_sample_input = train_data_inputs[0:int((0.8) * data.shape[0])]\n",
    "# train_sample_output = train_data_outputs[0:int((0.8) * data.shape[0])]\n",
    "\n",
    "# test_sample_input = train_data_inputs[int((0.8) * data.shape[0]) : data.shape[0]]\n",
    "# test_sample_output = train_data_outputs[int((0.8) * data.shape[0]) : data.shape[0]]\n",
    "Train = my_data[1:3750,:]\n",
    "Test  = my_data[3750:5000,:]\n",
    "Train_X = Train[:,0:20]\n",
    "Train_y =Train[:,20:23]\n",
    "Test_X = Test[:,0:20]\n",
    "Test_y =  Test[:,20:23]\n",
    "# print(Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Train_y.shape)\n",
    "print(Test_y.shape)\n",
    "print(Test_y[1200][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep Output data\n",
    "# train_sample_output_vector = np.asarray(prep_output(train_sample_output, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Data\n",
    "def pre_process(train_sample_input):\n",
    "\n",
    "    # Remove unncessary features\n",
    "    temp = train_sample_input\n",
    "    final = []\n",
    "    mini = [temp.min(axis=0)]\n",
    "    maxi = [temp.max(axis=0)]\n",
    "    diff = np.subtract(maxi, mini)\n",
    "    remove_idx = np.where(diff == 0)[1]\n",
    "    new_temp = np.delete(temp, remove_idx, 1)\n",
    "\n",
    "    # Normalize\n",
    "    mini = [new_temp.min(axis=0)]\n",
    "    maxi = [new_temp.max(axis=0)]\n",
    "    diff = np.subtract(maxi, mini)\n",
    "    for row in new_temp:\n",
    "        row = (np.subtract(row, mini))/(np.subtract(maxi, mini) + 1)\n",
    "        final.append(row)\n",
    "    preproc_train_sample = np.asarray(final)[:,0,:]\n",
    "    return preproc_train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_output(y, no_of_classes):\n",
    "    output = []\n",
    "    for i in y:\n",
    "        arr = np.zeros(no_of_classes)\n",
    "        arr[int(i)] = 1\n",
    "        output.append(arr)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_der(x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def ReLU(x):\n",
    "    x[x < 0] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_der(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_der(x):\n",
    "    return 1.0 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x.T) / np.sum(np.exp(x.T), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Layer Weights:\n",
    "def init_weights(hidden_layers, nodes_in_layer, input_count, output_count=3):\n",
    "    weight_array = []\n",
    "    np.random.seed(2)\n",
    "    for i in range(hidden_layers + 1):\n",
    "        if(i == 0):\n",
    "            weight_array.append(2 * np.random.random((input_count, nodes_in_layer[i])) - 1)\n",
    "        elif(i == hidden_layers):\n",
    "            weight_array.append(2 * np.random.random((nodes_in_layer[i - 1], output_count)) - 1)\n",
    "        else:\n",
    "            weight_array.append(2 * np.random.random((nodes_in_layer[i - 1], nodes_in_layer[i])) - 1)\n",
    "\n",
    "    return weight_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input_array(x):\n",
    "    n = x.shape[0]\n",
    "    m = x.shape[1]\n",
    "    padded_array = np.zeros(shape=(n, (m + 1))) + 1\n",
    "    padded_array[:,1:] = x\n",
    "    return padded_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hidden_layers_count, nodes_in_layer, number_of_training_iterations, input_data, output_data, alpha=8 * 10e-5, activation=\"tanh\"):\n",
    "    loss_function = []\n",
    "    n = hidden_layers_count + 1\n",
    "    input_count = input_data.shape[1]\n",
    "    output_count = output_data.shape[1]\n",
    "    weight_arr = init_weights(hidden_layers_count, nodes_in_layer, input_count)\n",
    "    output = []\n",
    "    for m in range(number_of_training_iterations + 1):\n",
    "        \n",
    "        layer_output = []\n",
    "        layer_output.append(input_data)\n",
    "        for i in range(0, n):\n",
    "            if(activation == \"sigmoid\"):\n",
    "                layer_output.append(sigmoid(np.dot(layer_output[i], weight_arr[i])))\n",
    "            elif(activation == \"ReLU\"):\n",
    "                layer_output.append(ReLU(np.dot(layer_output[i], weight_arr[i])))\n",
    "            elif(activation == \"tanh\"):\n",
    "                layer_output.append(tanh(np.dot(layer_output[i], weight_arr[i])))\n",
    "        layer_output.append(tanh(np.dot(layer_output[i], weight_arr[hidden_layers_count])))\n",
    "        if(m == number_of_training_iterations):\n",
    "            output = layer_output\n",
    "            f=open('weights_test.out','ab')\n",
    "            for data_slice in weight_arr:\n",
    "                np.savetxt(f, data_slice)\n",
    "            break\n",
    "\n",
    "        delta_arr = [None] * n\n",
    "        error_arr = [None] * n\n",
    "        for i in range(hidden_layers_count, -1, -1):\n",
    "            if(i == hidden_layers_count):\n",
    "                error_arr[i] = output_data - layer_output[i + 2]\n",
    "            else:\n",
    "                error_arr[i] = delta_arr[i + 1].dot(weight_arr[i + 1].T)\n",
    "            if(activation == \"sigmoid\"):\n",
    "                delta_arr[i] = error_arr[i] * sigmoid_der(layer_output[i + 1])\n",
    "            elif(activation == \"ReLU\"):\n",
    "                delta_arr[i] = error_arr[i] * ReLU_der(layer_output[i + 1])\n",
    "            elif(activation == \"tanh\"):\n",
    "                delta_arr[i] = error_arr[i] * tanh_der(layer_output[i + 1])\n",
    "\n",
    "        loss_function.append(cross_entropy(layer_output[hidden_layers_count + 1], output_data))\n",
    "        if(m%100 == 0):\n",
    "            print(\"Error: \", str(np.mean(np.abs(error_arr[hidden_layers_count]))))\n",
    "            \n",
    "        for i in range(0, n):\n",
    "            weight_arr[i] += alpha * layer_output[i].T.dot(delta_arr[i])\n",
    "    \n",
    "    plt.plot(range(1, number_of_training_iterations + 1), loss_function)\n",
    "    return output[hidden_layers_count + 1], weight_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax the output\n",
    "def final_output(inp):\n",
    "    softmax_op = tanh(inp).T\n",
    "    length_op = len(softmax_op)\n",
    "    \n",
    "    \n",
    "    output_list = [softmax_op[i].argmax(axis=0) for i in range(length_op)]\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproc subset with varying iteration count\n",
    "# preproc_train_example = pre_process(train_sample_input)\n",
    "# padded_input_example = pad_input_array(preproc_train_example)\n",
    "\n",
    "\n",
    "weight_arr = train(hidden_layers, nodes_in_layer, 10000, Train_X, Train_y,alpha= 10e-5, activation=\"tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0017344515001842286 Sigmoid 15 \n",
    "-9.989999933407882e-10 ReLU 15\n",
    "0.23845533036627764 tanh 15\n",
    "\n",
    "\n",
    "Error:  0.16577812970189867 ReLU 18 500_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(np.where(train_sample_output - final_output(output[0]) == 0)).shape(1))\n",
    "print(train_sample_output.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,5), error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weight_arr, input_data, hidden_layers_count, activation=\"tanh\"):\n",
    "    layer_output = []\n",
    "    layer_output.append(input_data)\n",
    "    for i in range(0, hidden_layers_count + 1):\n",
    "        layer_output.append(tanh(np.dot(layer_output[i], weight_arr[i])))\n",
    "    layer_output.append(np.asarray(tanh(layer_output[hidden_layers_count + 1])).T)\n",
    "    return layer_output[hidden_layers_count + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation_data = np.genfromtxt(\"Apparel/apparel-trainval.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "pad_data = pad_input_array(Test_X)\n",
    "final = predict(weight_arr, pad_data, 1, activation=\"tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"test_output_final_final_OP_new.csv\", final_output(final), newline=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(np.where(train_sample_output[0:300] - final_output(predict(weight_arr[1], data[0:300], 1, activation=\"sigmoid\")) == 0)).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN\n",
    "op_1 = weight_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(np.where(train_sample_output[0:500] - final_output(predict(op_1[1], pad_input_array(train_sample_input[0:500]), 1, activation=\"sigmoid\")) == 0)).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN - 2\n",
    "op_2 = weight_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(np.where(train_sample_output - final_output(predict(weight_arr[1], pad_input_array(train_sample_input), 1, activation=\"sigmoid\")) == 0)).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = final_output(predict(weight_arr[1], pad_input_array(train_sample_input), 1, activation=\"sigmoid\"))\n",
    "np.savetxt(\"20161116_prediction.csv\", final_prediction, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.asarray(train_sample_output - final_output(predict(weight_arr[1], pad_input_array(train_sample_input), 1, activation=\"sigmoid\")))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [[1 , 2 , 3, 4], [1 , 2 , 3, 4], [1 , 2 , 3, 4]]\n",
    "t= np.asarray(t)\n",
    "print(t.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final_op = np.asarray(final_output(predict(weight_arr[1], pad_input_array(train_sample_input), 1, activation=\"sigmoid\")))\n",
    "print(final_op)\n",
    "# pd.DataFrame(final_op.T).to_csv(\"20161116_prediction_2.csv\", header=None, index=None)\n",
    "np.savetxt(\"20161116_prediction_3.csv\", final_op.astype(int), newline=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
